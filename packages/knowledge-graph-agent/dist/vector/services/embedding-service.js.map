{"version":3,"file":"embedding-service.js","sources":["../../../src/vector/services/embedding-service.ts"],"sourcesContent":["/**\n * Embedding Service\n *\n * Provides text embedding generation using transformer models via @xenova/transformers.\n * Uses the all-MiniLM-L6-v2 model (384 dimensions) by default for efficient local inference.\n *\n * @module vector/services/embedding-service\n */\n\nimport { pipeline, type Pipeline, type FeatureExtractionPipeline } from '@xenova/transformers';\nimport { createLogger } from '../../utils/index.js';\n\nconst logger = createLogger('embedding-service');\n\n/**\n * Configuration for the embedding service\n */\nexport interface EmbeddingConfig {\n  /**\n   * Model identifier for the embedding model\n   * @default 'Xenova/all-MiniLM-L6-v2'\n   */\n  model: string;\n\n  /**\n   * Expected output dimensions for embeddings\n   * @default 384\n   */\n  dimensions: number;\n\n  /**\n   * Maximum input text length in characters (not tokens)\n   * Longer texts will be truncated\n   * @default 512\n   */\n  maxLength: number;\n\n  /**\n   * Number of texts to process in a single batch\n   * @default 32\n   */\n  batchSize: number;\n\n  /**\n   * Whether to normalize embeddings to unit length\n   * @default true\n   */\n  normalize: boolean;\n\n  /**\n   * Pooling strategy for generating embeddings from token outputs\n   * @default 'mean'\n   */\n  pooling: 'mean' | 'cls' | 'none';\n\n  /**\n   * Enable quantization for reduced memory usage\n   * @default false\n   */\n  quantized: boolean;\n}\n\n/**\n * Default configuration for embedding service\n * Uses all-MiniLM-L6-v2 which provides a good balance of quality and speed\n */\nconst DEFAULT_CONFIG: EmbeddingConfig = {\n  model: 'Xenova/all-MiniLM-L6-v2',\n  dimensions: 384,\n  maxLength: 512,\n  batchSize: 32,\n  normalize: true,\n  pooling: 'mean',\n  quantized: false,\n};\n\n/**\n * Result of embedding generation\n */\nexport interface EmbeddingResult {\n  /**\n   * The embedding vector\n   */\n  embedding: Float32Array;\n\n  /**\n   * Time taken to generate the embedding in milliseconds\n   */\n  durationMs: number;\n\n  /**\n   * Number of input tokens (approximate)\n   */\n  tokenCount?: number;\n}\n\n/**\n * Result of batch embedding generation\n */\nexport interface BatchEmbeddingResult {\n  /**\n   * Array of embedding vectors\n   */\n  embeddings: Float32Array[];\n\n  /**\n   * Total time taken in milliseconds\n   */\n  durationMs: number;\n\n  /**\n   * Number of texts successfully embedded\n   */\n  successCount: number;\n\n  /**\n   * Number of texts that failed\n   */\n  errorCount: number;\n\n  /**\n   * Error details for failed texts\n   */\n  errors?: Array<{ index: number; error: string }>;\n}\n\n/**\n * Embedding Service\n *\n * Provides text embedding generation using transformer models.\n * Supports single and batch operations with automatic initialization.\n *\n * @example\n * ```typescript\n * const service = new EmbeddingService();\n * await service.initialize();\n *\n * // Single embedding\n * const embedding = await service.embed('Hello, world!');\n *\n * // Batch embeddings\n * const embeddings = await service.embedBatch([\n *   'First document',\n *   'Second document',\n *   'Third document'\n * ]);\n * ```\n */\nexport class EmbeddingService {\n  private pipeline: FeatureExtractionPipeline | null = null;\n  private config: EmbeddingConfig;\n  private initPromise: Promise<void> | null = null;\n  private isInitialized: boolean = false;\n\n  /**\n   * Create a new EmbeddingService\n   *\n   * @param config - Optional configuration overrides\n   */\n  constructor(config: Partial<EmbeddingConfig> = {}) {\n    this.config = { ...DEFAULT_CONFIG, ...config };\n  }\n\n  /**\n   * Initialize the embedding pipeline\n   *\n   * Loads the transformer model. This is called automatically on first use\n   * but can be called explicitly to pre-load the model.\n   *\n   * @throws Error if model loading fails\n   */\n  async initialize(): Promise<void> {\n    if (this.isInitialized) {\n      return;\n    }\n\n    if (this.initPromise) {\n      return this.initPromise;\n    }\n\n    this.initPromise = this.doInitialize();\n    return this.initPromise;\n  }\n\n  private async doInitialize(): Promise<void> {\n    try {\n      logger.info('Loading embedding model', { model: this.config.model });\n      const startTime = Date.now();\n\n      // Load the feature extraction pipeline\n      this.pipeline = (await pipeline(\n        'feature-extraction',\n        this.config.model,\n        { quantized: this.config.quantized }\n      )) as FeatureExtractionPipeline;\n\n      const durationMs = Date.now() - startTime;\n      logger.info('Embedding model loaded', {\n        model: this.config.model,\n        durationMs,\n      });\n\n      this.isInitialized = true;\n    } catch (error) {\n      this.initPromise = null;\n      const message = error instanceof Error ? error.message : String(error);\n      logger.error('Failed to load embedding model', new Error(message), {\n        model: this.config.model,\n      });\n      throw new Error(`Failed to initialize embedding service: ${message}`);\n    }\n  }\n\n  /**\n   * Generate embedding for a single text\n   *\n   * @param text - Text to embed\n   * @returns Embedding result with vector and metadata\n   * @throws Error if not initialized or embedding fails\n   */\n  async embed(text: string): Promise<EmbeddingResult> {\n    await this.initialize();\n\n    if (!this.pipeline) {\n      throw new Error('Embedding pipeline not initialized');\n    }\n\n    const startTime = Date.now();\n\n    // Truncate text if too long\n    const truncated = this.truncateText(text);\n\n    try {\n      const output = await this.pipeline(truncated, {\n        pooling: this.config.pooling,\n        normalize: this.config.normalize,\n      });\n\n      // Extract the embedding data\n      const embedding = this.extractEmbedding(output);\n\n      const durationMs = Date.now() - startTime;\n\n      logger.debug('Generated embedding', {\n        inputLength: text.length,\n        truncated: text.length !== truncated.length,\n        durationMs,\n      });\n\n      return {\n        embedding,\n        durationMs,\n        tokenCount: Math.ceil(truncated.length / 4), // Rough estimate\n      };\n    } catch (error) {\n      const message = error instanceof Error ? error.message : String(error);\n      logger.error('Embedding generation failed', new Error(message));\n      throw new Error(`Failed to generate embedding: ${message}`);\n    }\n  }\n\n  /**\n   * Generate embeddings for multiple texts\n   *\n   * Processes texts in batches for efficiency.\n   *\n   * @param texts - Array of texts to embed\n   * @returns Batch result with embeddings and metadata\n   */\n  async embedBatch(texts: string[]): Promise<BatchEmbeddingResult> {\n    await this.initialize();\n\n    if (!this.pipeline) {\n      throw new Error('Embedding pipeline not initialized');\n    }\n\n    const startTime = Date.now();\n    const results: Float32Array[] = [];\n    const errors: Array<{ index: number; error: string }> = [];\n\n    // Process in batches\n    for (let i = 0; i < texts.length; i += this.config.batchSize) {\n      const batch = texts.slice(i, i + this.config.batchSize);\n      const truncatedBatch = batch.map((t) => this.truncateText(t));\n\n      try {\n        // Process batch\n        const outputs = await this.pipeline(truncatedBatch, {\n          pooling: this.config.pooling,\n          normalize: this.config.normalize,\n        });\n\n        // Extract embeddings from batch output\n        const batchEmbeddings = this.extractBatchEmbeddings(outputs, batch.length);\n        results.push(...batchEmbeddings);\n      } catch (error) {\n        // Fall back to individual processing on batch error\n        logger.warn('Batch embedding failed, falling back to individual', {\n          batchIndex: i / this.config.batchSize,\n        });\n\n        for (let j = 0; j < batch.length; j++) {\n          try {\n            const output = await this.pipeline(truncatedBatch[j], {\n              pooling: this.config.pooling,\n              normalize: this.config.normalize,\n            });\n            results.push(this.extractEmbedding(output));\n          } catch (individualError) {\n            errors.push({\n              index: i + j,\n              error: individualError instanceof Error ? individualError.message : String(individualError),\n            });\n            // Push zero vector as placeholder\n            results.push(new Float32Array(this.config.dimensions));\n          }\n        }\n      }\n    }\n\n    const durationMs = Date.now() - startTime;\n\n    logger.info('Batch embedding completed', {\n      totalTexts: texts.length,\n      successCount: texts.length - errors.length,\n      errorCount: errors.length,\n      durationMs,\n      avgTimePerText: durationMs / texts.length,\n    });\n\n    return {\n      embeddings: results,\n      durationMs,\n      successCount: texts.length - errors.length,\n      errorCount: errors.length,\n      errors: errors.length > 0 ? errors : undefined,\n    };\n  }\n\n  /**\n   * Truncate text to maximum length\n   *\n   * @param text - Text to truncate\n   * @returns Truncated text\n   */\n  private truncateText(text: string): string {\n    if (text.length <= this.config.maxLength * 4) {\n      return text;\n    }\n    // Truncate at word boundary if possible\n    const truncated = text.slice(0, this.config.maxLength * 4);\n    const lastSpace = truncated.lastIndexOf(' ');\n    if (lastSpace > this.config.maxLength * 2) {\n      return truncated.slice(0, lastSpace);\n    }\n    return truncated;\n  }\n\n  /**\n   * Extract embedding from pipeline output\n   *\n   * @param output - Pipeline output tensor\n   * @returns Float32Array embedding\n   */\n  private extractEmbedding(output: unknown): Float32Array {\n    // Handle different output formats from transformers.js\n    if (output && typeof output === 'object') {\n      const outputObj = output as Record<string, unknown>;\n\n      // Check for .data property (Tensor object)\n      if ('data' in outputObj && outputObj.data) {\n        const data = outputObj.data;\n        if (data instanceof Float32Array) {\n          return data.slice(0, this.config.dimensions);\n        }\n        if (ArrayBuffer.isView(data)) {\n          return new Float32Array(data.buffer, data.byteOffset, this.config.dimensions);\n        }\n        if (Array.isArray(data)) {\n          return new Float32Array(data.slice(0, this.config.dimensions));\n        }\n      }\n\n      // Check for nested array structure\n      if (Array.isArray(outputObj)) {\n        const flattened = this.flattenArray(outputObj);\n        return new Float32Array(flattened.slice(0, this.config.dimensions));\n      }\n    }\n\n    throw new Error('Unexpected output format from embedding pipeline');\n  }\n\n  /**\n   * Extract multiple embeddings from batch output\n   *\n   * @param output - Pipeline output tensor for batch\n   * @param batchSize - Number of items in the batch\n   * @returns Array of embeddings\n   */\n  private extractBatchEmbeddings(output: unknown, batchSize: number): Float32Array[] {\n    const results: Float32Array[] = [];\n\n    if (output && typeof output === 'object') {\n      const outputObj = output as Record<string, unknown>;\n\n      if ('data' in outputObj && outputObj.data) {\n        const data = outputObj.data;\n        let dataArray: Float32Array;\n\n        if (data instanceof Float32Array) {\n          dataArray = data;\n        } else if (ArrayBuffer.isView(data)) {\n          dataArray = new Float32Array(data.buffer, data.byteOffset);\n        } else if (Array.isArray(data)) {\n          dataArray = new Float32Array(data as number[]);\n        } else {\n          throw new Error('Unexpected data format in batch output');\n        }\n\n        // Split the data array into individual embeddings\n        for (let i = 0; i < batchSize; i++) {\n          const start = i * this.config.dimensions;\n          const embedding = dataArray.slice(start, start + this.config.dimensions);\n          results.push(embedding);\n        }\n\n        return results;\n      }\n    }\n\n    throw new Error('Unexpected output format from batch embedding pipeline');\n  }\n\n  /**\n   * Flatten nested array\n   *\n   * @param arr - Array to flatten\n   * @returns Flattened number array\n   */\n  private flattenArray(arr: unknown[]): number[] {\n    const result: number[] = [];\n    for (const item of arr) {\n      if (Array.isArray(item)) {\n        result.push(...this.flattenArray(item));\n      } else if (typeof item === 'number') {\n        result.push(item);\n      }\n    }\n    return result;\n  }\n\n  /**\n   * Get the embedding dimensions\n   *\n   * @returns Number of dimensions in generated embeddings\n   */\n  getDimensions(): number {\n    return this.config.dimensions;\n  }\n\n  /**\n   * Get the current configuration\n   *\n   * @returns Copy of the current configuration\n   */\n  getConfig(): EmbeddingConfig {\n    return { ...this.config };\n  }\n\n  /**\n   * Check if the service is initialized\n   *\n   * @returns True if initialized\n   */\n  isReady(): boolean {\n    return this.isInitialized;\n  }\n\n  /**\n   * Get model information\n   *\n   * @returns Model details\n   */\n  getModelInfo(): { model: string; dimensions: number; quantized: boolean } {\n    return {\n      model: this.config.model,\n      dimensions: this.config.dimensions,\n      quantized: this.config.quantized,\n    };\n  }\n}\n\n/**\n * Create an embedding service instance\n *\n * Factory function for creating embedding services.\n *\n * @param config - Optional configuration overrides\n * @returns New EmbeddingService instance\n *\n * @example\n * ```typescript\n * // Default configuration (all-MiniLM-L6-v2, 384 dimensions)\n * const service = createEmbeddingService();\n *\n * // Custom configuration\n * const customService = createEmbeddingService({\n *   model: 'Xenova/all-MiniLM-L12-v2',\n *   dimensions: 384,\n *   batchSize: 64,\n * });\n * ```\n */\nexport function createEmbeddingService(config?: Partial<EmbeddingConfig>): EmbeddingService {\n  return new EmbeddingService(config);\n}\n\n/**\n * Default embedding service singleton\n * Created lazily on first use\n */\nlet defaultService: EmbeddingService | null = null;\n\n/**\n * Get the default embedding service\n *\n * Returns a shared singleton instance for convenience.\n * Use createEmbeddingService() for custom configurations.\n *\n * @returns Default EmbeddingService instance\n */\nexport function getDefaultEmbeddingService(): EmbeddingService {\n  if (!defaultService) {\n    defaultService = new EmbeddingService();\n  }\n  return defaultService;\n}\n"],"names":[],"mappings":";;;;;;;;AAYe,aAAa,mBAAmB;"}