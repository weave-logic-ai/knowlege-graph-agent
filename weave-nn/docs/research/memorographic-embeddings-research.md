---
visual:
  icon: ðŸ”¬
icon: ðŸ”¬
---
# Memorographic Embeddings Research Report

**Date:** January 2025
**Research Focus:** Memory-specific vector representation strategies for learning loop systems
**Sources:** Latest 2024-2025 research on memory-augmented LLMs

---

## ðŸ“š Related Documentation

### Memory & Learning Loop
- [[PHASE-12-LEARNING-LOOP-BLUEPRINT]] - Learning loop architecture
- [[PHASE-12-LEARNING-LOOP-INTEGRATION]] - Learning loop integration
- [[WEAVER-COMPLETE-IMPLEMENTATION-GUIDE]] - Complete Weaver implementation

### Chunking & Vector Storage
- [[CHUNKING-STRATEGY-SYNTHESIS]] - Chunking strategy design
- [[CHUNKING-IMPLEMENTATION-DESIGN]] - Chunking implementation
- [[VECTOR-DB-MARKDOWN-WORKFLOW-ARCHITECTURE]] - Vector database architecture

### Phase 13 Integration
- [[phase-13-master-plan]] - Phase 13 overall plan
- [[chunking-strategies-research-2024-2025]] - Modern chunking research

### See Also
- [[MARKDOWN-ASYNC-WORKFLOW-ARCHITECTURE]] - Markdown workflows
- [[USER-FEEDBACK-REFLECTION-DESIGN]] - User feedback patterns

---

## Executive Summary

This research report provides comprehensive analysis and implementable recommendations for building a learning loop system with specialized memory embeddings. Unlike standard document embeddings designed for static retrieval, memorographic embeddings must capture temporal dynamics, hierarchical relationships, user preferences, and experiential patterns.

### Key Findings

1. **Memory embeddings require fundamentally different approaches** than document embeddings due to temporal, hierarchical, and experiential dimensions
2. **Hierarchical memory architectures** (2024-2025) show superior performance for complex reasoning and long-context scenarios
3. **Contextual Experience Replay** enables LLM agents to learn from past interactions without retraining
4. **Hybrid retrieval strategies** (semantic + keyword + temporal) dramatically improve memory recall accuracy
5. **Metadata enrichment** is critical for distinguishing between episodic events, learned patterns, and user preferences

---

## 1. Episodic Memory Embeddings

### Overview
Episodic memory captures specific experiences and events with spatio-temporal context. This is essential for storing task execution experiences and decision trails.

### 2024-2025 Research Highlights

#### Video-EM Framework (2024)
- **Components:**
  - Adaptive Event Expansion (AEE)
  - Dynamic Scene Narratives (DSN)
  - Dynamic Scene Relationships (DSR)
- **Application:** Long-form video understanding with hierarchical event encoding
- **Key Insight:** Hierarchical text descriptions generated by vision-language models enable structured episodic representation

#### Episodic Memory for Cognitive Agents (EMCA)
- **Architecture:** Graph-based structure for incremental storage and retrieval
- **Approach:** Integrates real-world interactions through explicit relationship modeling
- **Benefit:** Allows agents to build experiential knowledge over time

#### GraphRAG Evolution
- **Transition:** From vector embedding databases to structured graphs
- **Advantage:** Explicitly encodes relationships as connections between nodes
- **Use Case:** Better for episodic memory where events have explicit temporal and causal relationships

### Implementation Recommendations for Task Execution Experiences

```yaml
episodic_memory_structure:
  event_representation:
    - temporal_context:
        timestamp: "ISO-8601 format"
        duration: "milliseconds"
        sequence_number: "integer"
    - spatial_context:
        workspace: "project/directory path"
        file_locations: ["absolute paths"]
        environment: "execution environment details"
    - entities:
        agents: ["agent IDs involved"]
        tools: ["tools used"]
        files: ["files modified"]
    - event_description:
        action_type: "task|edit|review|test|deploy"
        natural_language_summary: "human-readable description"
        structured_data: "JSON of key-value pairs"
    - relationships:
        caused_by: "parent event ID"
        leads_to: ["child event IDs"]
        related_to: ["semantically related event IDs"]

  embedding_strategy:
    primary_embedding: "dense vector from task description + context"
    temporal_embedding: "time-aware representation (T-Rep or TEMPO style)"
    relationship_embedding: "graph structure embedding"
    composite_embedding: "weighted combination based on retrieval type"

  chunking_approach:
    granularity: "per-action (fine) or per-task (coarse)"
    context_window: "include N prior and subsequent events"
    semantic_boundaries: "identify task completion boundaries"
```

### Specialized Preprocessing

1. **Event Boundary Detection**
   - Use LLM-based chunking to identify logical task boundaries
   - Apply meta-chunking to group related sub-actions
   - Preserve hierarchical structure (task > subtask > action)

2. **Temporal Annotation**
   - Add absolute timestamps and relative time deltas
   - Include sequence numbers for ordering
   - Mark event dependencies and causal relationships

3. **Context Enrichment**
   - Generate contextual summaries using Claude (Anthropic's Contextual Retrieval approach)
   - Include "what happened before" and "what happened after" context
   - Add environmental state snapshots

### Query Strategies

1. **Temporal Queries**
   - "What did we do last week on this project?"
   - "Show me the sequence of events that led to this error"
   - Use temporal embeddings with time-range filtering

2. **Causal Queries**
   - "Why did we make this decision?"
   - "What were the consequences of action X?"
   - Use graph traversal with semantic similarity

3. **Pattern Queries**
   - "Find similar debugging sessions"
   - "What approaches have we tried for this type of problem?"
   - Use semantic similarity with event-type filtering

---

## 2. Semantic Memory Embeddings

### Overview
Semantic memory stores learned knowledge, concepts, and generalizations. This is critical for SOPs, workflows, and learned patterns.

### 2024-2025 Research Highlights

#### ReaLM (2024) - Residual Quantization
- **Approach:** Quantization-based mapping of entity embeddings into discrete codebook
- **Integration:** Codebook integrated into LLM tokenizer
- **Benefit:** Efficient knowledge representation with structural preservation

#### GraphTranslator (2024)
- **Innovation:** Alignment data generation for graph-to-token embedding conversion
- **Application:** Seamless integration of KG knowledge with LLM representations
- **Key Advantage:** Bridges explicit semantics of KGs with implicit knowledge of pre-trained models

#### Graphiti - Temporal Knowledge Graphs
- **Features:**
  - Tracks changes in facts and relationships over time
  - Point-in-time queries
  - Fusion of time, full-text, semantic, and graph algorithms
- **Use Case:** Evolving knowledge representation where facts change over time

### Implementation Recommendations for Learned Knowledge

```yaml
semantic_memory_structure:
  knowledge_representation:
    - concept_nodes:
        concept_id: "unique identifier"
        concept_type: "pattern|sop|workflow|best-practice|anti-pattern"
        description: "natural language description"
        formal_representation: "structured schema/JSON"
        abstraction_level: "specific|general|abstract"

    - relationship_edges:
        relationship_type: "is-a|part-of|requires|conflicts-with|similar-to"
        source_node: "concept ID"
        target_node: "concept ID"
        strength: "0.0-1.0 confidence score"
        temporal_validity: "when this relationship holds true"

    - knowledge_provenance:
        derived_from: ["episodic event IDs that led to this knowledge"]
        confidence: "0.0-1.0 based on evidence strength"
        last_updated: "timestamp"
        usage_count: "how often retrieved/applied"
        success_rate: "outcome tracking"

  embedding_strategy:
    concept_embedding: "dense semantic vector"
    structure_embedding: "graph position/role embedding"
    multi_hop_embedding: "neighborhood aggregation"
    hierarchical_embedding: "abstraction level encoding"

  chunking_approach:
    unit: "per-concept or per-SOP-step"
    hierarchical_levels:
      - abstract: "high-level principles"
      - intermediate: "specific workflows"
      - concrete: "individual steps"
    cross_references: "explicit links between related concepts"
```

### Specialized Preprocessing

1. **Knowledge Distillation**
   - Extract patterns from multiple episodic experiences
   - Use LLM to generalize from specific examples
   - Identify common sub-patterns in successful workflows

2. **Hierarchical Organization**
   - Build abstraction hierarchies (as in MemTree 2024)
   - Create multi-resolution representations
   - Enable both detail and summary retrieval

3. **Knowledge Graph Construction**
   - Use GraphRAG approach for explicit relationship modeling
   - Apply temporal knowledge graph techniques for evolving knowledge
   - Implement composite embeddings for multi-field representation

### Query Strategies

1. **Conceptual Queries**
   - "What do we know about REST API design?"
   - "Show me our testing workflow"
   - Use hierarchical retrieval starting at appropriate abstraction level

2. **Relational Queries**
   - "What are the prerequisites for deploying to production?"
   - "What conflicts with approach X?"
   - Use graph traversal with relationship-type filtering

3. **Example-Based Queries**
   - "How have we solved problems like this before?"
   - "Find SOPs similar to X"
   - Use hybrid semantic similarity + structural similarity

---

## 3. Preference Embeddings

### Overview
Preference embeddings capture user feedback, choices, and learned behaviors. Essential for personalization and adaptive learning.

### 2024-2025 Research Highlights

#### Variational Preference Learning (VPL)
- **Approach:** Compresses user preference information into concise probabilistic embedding vectors
- **Application:** RLHF for LLMs with separable embedding spaces
- **Benefit:** Distinguishes users with divergent preferences

#### Decomposed Reward Models (DRM)
- **Innovation:** Represents diverse human preferences as orthogonal basis vectors
- **Analysis:** Uses PCA to decompose preference vectors
- **Key Insight:** Human preferences can be represented as linear combinations of principal components

#### Neural Collaborative Filtering
- **Method:** Joint learning of user and item embeddings with preference relations
- **Property:** Users with similar preferences have nearby embeddings in learned space
- **Application:** Personalized recommendation and feedback integration

### Implementation Recommendations for User Preferences

```yaml
preference_memory_structure:
  preference_representation:
    - user_profile:
        user_id: "unique identifier"
        preference_vector: "dense embedding in preference space"
        basis_weights: "PCA-style decomposition into preference dimensions"
        preference_dimensions:
          - code_style: "verbose|concise|balanced"
          - testing_approach: "unit-first|integration-first|e2e-first"
          - documentation_detail: "minimal|moderate|comprehensive"
          - risk_tolerance: "conservative|balanced|aggressive"

    - feedback_events:
        event_id: "unique identifier"
        timestamp: "when feedback was given"
        context: "what was being evaluated"
        feedback_type: "positive|negative|neutral|correction"
        feedback_content: "user's input"
        preference_signal: "extracted preference indicator"
        embedding: "vector representation of feedback"

    - preference_evolution:
        snapshots: ["preference vectors at different time points"]
        drift_detection: "tracking changes over time"
        confidence: "how certain we are about each preference dimension"

  embedding_strategy:
    preference_embedding: "learned user representation in preference space"
    feedback_embedding: "individual feedback event vectors"
    context_preference_embedding: "conditional preferences based on task context"
    temporal_preference_embedding: "time-aware preference representation"

  learning_approach:
    online_learning: "update preferences incrementally with each feedback"
    batch_updates: "periodic retraining on feedback history"
    transfer_learning: "bootstrap from similar users or defaults"
    explainability: "track which feedback events influenced preferences"
```

### Specialized Preprocessing

1. **Feedback Extraction**
   - Parse explicit feedback (thumbs up/down, ratings, corrections)
   - Infer implicit feedback (accepted suggestions, ignored warnings, manual overrides)
   - Extract preference signals from natural language feedback

2. **Preference Dimensionality Reduction**
   - Apply PCA or similar to identify principal preference dimensions
   - Create interpretable preference axes
   - Use variational approach for probabilistic representation

3. **Context-Dependent Preferences**
   - Segment preferences by task type, time of day, project phase
   - Learn conditional preference distributions
   - Enable "in this context, user prefers X" retrieval

### Query Strategies

1. **Personalization Queries**
   - "Generate code in user's preferred style"
   - "Suggest next action based on user's typical workflow"
   - Use preference embedding to weight generation

2. **Preference-Based Filtering**
   - "Find examples the user has liked before"
   - "Avoid approaches the user has rejected"
   - Use similarity to positive/negative feedback embeddings

3. **Adaptive Behavior**
   - "Adjust verbosity based on user preference"
   - "Select testing strategy matching user's approach"
   - Use preference dimensions as configuration parameters

---

## 4. Temporal Embeddings

### Overview
Temporal embeddings incorporate time and sequence information, enabling time-aware retrieval and understanding of evolving patterns.

### 2024-2025 Research Highlights

#### T-Rep (2024) - Time-Embeddings for Time Series
- **Innovation:** Self-supervised method integrating time-embeddings in encoder
- **Components:** Encode trend, periodicity, and distribution shifts
- **Application:** Fine-grained time series representation learning

#### TEMPO (ICLR 2024)
- **Approach:** Soft prompts with learnable continuous vectors for temporal knowledge
- **Knowledge Types:** Trend and seasonality encoding
- **Benefit:** Efficient GPT tuning for forecasting tasks

#### Temporal Dynamic Embedding (TDE)
- **Capability:** Handle variable numbers of features over time
- **Method:** Treat each time series variable as evolving embedding vector
- **Use Case:** Systems where available information changes over time

#### Relative Embeddings in Transformers
- **Innovation:** Redesigned relative position representations
- **Detection:** Latent spatial, temporal, and spatio-temporal dependencies
- **Performance:** More effective than previous Transformer-based models

### Implementation Recommendations for Time-Aware Memory

```yaml
temporal_memory_structure:
  time_representation:
    - absolute_time:
        timestamp: "ISO-8601 datetime"
        date_encoding: "cyclical encoding of day/month/year"
        time_of_day_encoding: "cyclical encoding of hour/minute"

    - relative_time:
        time_since_event: "milliseconds from reference point"
        sequence_position: "integer position in sequence"
        session_time: "time within current session"

    - temporal_patterns:
        trend: "increasing|decreasing|stable"
        periodicity: "daily|weekly|monthly cycles detected"
        seasonality: "time-of-day or day-of-week patterns"
        recency: "how fresh this information is"

  embedding_strategy:
    base_embedding: "content semantic vector"
    temporal_embedding: "time-aware representation using T-Rep approach"
    positional_embedding: "relative position in sequence"
    recency_weighting: "decay function for older memories"
    temporal_fusion: "combine content + time embeddings"

  time_decay_models:
    exponential_decay: "weight = exp(-lambda * time_delta)"
    learned_decay: "train decay parameters per memory type"
    contextual_decay: "different decay rates for different information types"
    no_decay_for: ["core knowledge", "SOPs", "stable preferences"]
```

### Specialized Preprocessing

1. **Time Feature Engineering**
   - Extract cyclical features (hour, day of week, month)
   - Compute relative time deltas to important events
   - Identify temporal patterns in sequences

2. **Temporal Normalization**
   - Handle irregular time intervals
   - Resample to consistent time grid if needed
   - Use TDE approach for variable-length sequences

3. **Trend and Seasonality Extraction**
   - Decompose time series into trend, seasonal, and residual components
   - Encode each component separately
   - Use TEMPO-style learnable temporal prompts

### Query Strategies

1. **Recency-Aware Queries**
   - "What have we learned recently about X?"
   - "Show me the latest version of this workflow"
   - Combine semantic similarity with recency weighting

2. **Temporal Range Queries**
   - "What did we work on last week?"
   - "Show me all events between timestamps A and B"
   - Use temporal filtering with semantic search

3. **Pattern-Based Queries**
   - "What do we typically do on Monday mornings?"
   - "Find the recurring pattern in our deployment process"
   - Use periodicity detection with sequence matching

4. **Point-in-Time Queries**
   - "What did we know about X at timestamp T?"
   - "Reconstruct the state of the project as of date D"
   - Use temporal knowledge graph point-in-time retrieval (Graphiti approach)

---

## 5. Hierarchical Memory Architecture

### Overview
Hierarchical memory organizes information at multiple levels of abstraction, enabling efficient retrieval from detailed specifics to high-level summaries.

### 2024-2025 Research Highlights

#### MemTree (2024) - Dynamic Tree Memory
- **Structure:** Tree with nodes at varying abstraction levels
- **Node Contents:** Aggregated textual content + semantic embeddings
- **Algorithm:** Dynamically adapts structure by comparing embeddings
- **Benefit:** Handles complex reasoning more effectively than flat lookup tables

#### H-MEM (2024) - Hierarchical Memory for LLM Agents
- **Architecture:** Multi-level memory storage with positional index encoding
- **Organization:** Structured and systematic at each layer
- **Retrieval:** Efficient and orderly, reducing irrelevant information
- **Performance:** Significantly reduces computational costs

#### Hierarchical Embedding Augmentation (2025)
- **Innovation:** Multi-layered embeddings with varying semantic granularity
- **Framework:** Multi-resolution context understanding
- **Levels:** Micro (token-level) to macro (document-level)
- **Application:** Empowers LLMs to refine understanding at multiple scales

#### Hierarchical Memory Transformer (HMT)
- **Approach:** Memory-augmented segment-level recurrence
- **Organization:** Multi-level memory hierarchy mimicking human behavior
- **Use Case:** Long-context processing

#### LEGOMem - Modular Procedural Memory
- **Structure:** Full-task memories and subtask memories
- **Storage:** Memory bank indexed by semantic embeddings
- **Reuse:** Augment planning and execution at inference time
- **Benefit:** Distills successful executions into reusable modules

### Implementation Recommendations for Multi-Level Memory

```yaml
hierarchical_memory_structure:
  memory_levels:
    level_0_atomic:
      description: "Individual actions, file edits, single decisions"
      granularity: "finest detail"
      retention: "shorter term (days-weeks)"
      embedding_dimension: 768
      examples:
        - "Changed variable name from x to userId"
        - "Added error handling to function parseInput"

    level_1_episodic:
      description: "Complete tasks, debugging sessions, code reviews"
      granularity: "task-level aggregation"
      retention: "medium term (weeks-months)"
      embedding_dimension: 1024
      aggregation: "summarize level_0 events"
      examples:
        - "Implemented user authentication feature"
        - "Debugged memory leak in WebSocket handler"

    level_2_semantic:
      description: "Patterns, workflows, learned approaches"
      granularity: "generalized knowledge"
      retention: "long term (months-years)"
      embedding_dimension: 1536
      abstraction: "extract patterns from level_1 tasks"
      examples:
        - "Standard workflow for adding new API endpoint"
        - "Debugging pattern for race conditions"

    level_3_strategic:
      description: "Principles, preferences, architectural decisions"
      granularity: "highest abstraction"
      retention: "permanent"
      embedding_dimension: 2048
      synthesis: "distill from level_2 patterns"
      examples:
        - "Prefer composition over inheritance"
        - "Always write tests before implementation"

  hierarchical_indexing:
    positional_encoding: "encode level and position within level"
    parent_child_links: "explicit pointers between levels"
    sibling_links: "connections within same level"
    cross_references: "semantic links across levels"

  retrieval_strategy:
    top_down:
      - start: "level_3 strategic"
      - filter: "narrow to relevant principles"
      - descend: "drill down to level_2, then level_1, then level_0"
      - benefit: "efficient for broad queries"

    bottom_up:
      - start: "level_0 atomic"
      - aggregate: "combine related atomic memories"
      - ascend: "build up to patterns and principles"
      - benefit: "useful for detail-oriented queries"

    parallel_search:
      - approach: "search all levels simultaneously"
      - combine: "merge results with level-appropriate weighting"
      - benefit: "comprehensive retrieval"

  memory_consolidation:
    frequency: "periodic (e.g., daily)"
    process:
      - identify_patterns: "find recurring sequences in level_0/level_1"
      - create_level_2: "distill patterns into semantic memory"
      - update_level_3: "refine strategic principles based on outcomes"
      - prune_level_0: "archive or delete low-value atomic memories"
```

### Specialized Preprocessing

1. **Hierarchical Summarization**
   - Generate summaries at each level using LLM
   - Preserve key details while increasing abstraction
   - Maintain bidirectional links between levels

2. **Dynamic Structure Adaptation**
   - Use MemTree algorithm to reorganize based on access patterns
   - Split nodes that are too broad
   - Merge nodes that are too granular
   - Rebalance tree based on retrieval frequency

3. **Multi-Resolution Embeddings**
   - Create separate embeddings for each level
   - Use different embedding dimensions appropriate to abstraction level
   - Apply hierarchical embedding augmentation for multi-scale context

### Query Strategies

1. **Adaptive Depth Retrieval**
   - Start at appropriate level based on query type
   - Expand to finer detail if needed
   - Summarize if too detailed
   - Example: "How do we handle authentication?" â†’ Start at level_2 (semantic patterns)

2. **Cross-Level Queries**
   - "Show me the principle and an example"
   - Retrieve from level_3 and level_1 simultaneously
   - Example: "Why do we prefer composition? Show me a case where we used it."

3. **Progressive Refinement**
   - Initial query returns level_2 or level_3 results
   - User asks for more detail â†’ descend to level_1 or level_0
   - User asks for summary â†’ ascend to higher level

---

## 6. Key Differences from Standard Document Embeddings

### Structural Differences

| Aspect | Document Embeddings | Memory Embeddings |
|--------|-------------------|------------------|
| **Temporal Dimension** | Static, time-invariant | Time-aware, decay functions, sequence encoding |
| **Relationships** | Implicit through similarity | Explicit through graph structure |
| **Granularity** | Fixed chunking (paragraphs) | Multi-level (atomic â†’ strategic) |
| **Metadata** | Minimal (source, date) | Rich (provenance, confidence, usage, outcomes) |
| **Update Pattern** | Infrequent, batch reindexing | Continuous, incremental learning |
| **Retrieval Strategy** | Pure semantic similarity | Hybrid (semantic + temporal + graph + preference) |
| **Purpose** | Find relevant passages | Enable learning and adaptation |
| **Abstraction** | Single level | Hierarchical (details â†’ patterns â†’ principles) |

### Functional Differences

1. **Documents are consumed; memories are grown**
   - Documents: indexed once, retrieved many times
   - Memories: created from experience, refined over time, consolidated into knowledge

2. **Documents are independent; memories are connected**
   - Documents: weakly linked (maybe cross-references)
   - Memories: densely connected (causal, temporal, semantic relationships)

3. **Documents are objective; memories are subjective**
   - Documents: same for all users
   - Memories: personalized, preference-weighted, context-dependent

4. **Documents are facts; memories are experiences + learning**
   - Documents: "here is information"
   - Memories: "we tried X, learned Y, now we know Z"

### Embedding Architecture Differences

```yaml
document_embedding:
  input: "static text chunk"
  preprocessing: "cleanup, tokenization"
  embedding: "dense vector (e.g., 768d)"
  metadata: "source, title, date"
  indexing: "flat or HNSW for ANN search"
  retrieval: "cosine similarity"

memory_embedding:
  input: "experience, event, or knowledge"
  preprocessing:
    - temporal_annotation
    - relationship_extraction
    - context_enrichment
    - abstraction_level_assignment
  embedding:
    base_vector: "semantic content (768d)"
    temporal_vector: "time representation (128d)"
    preference_vector: "user-specific weighting (64d)"
    structural_vector: "graph position (64d)"
    composite: "learned fusion (1024d)"
  metadata:
    provenance: ["source events"]
    confidence: 0.85
    usage_count: 47
    success_rate: 0.92
    last_updated: "timestamp"
    abstraction_level: 2
  indexing:
    semantic_index: "vector DB"
    temporal_index: "time-series DB"
    graph_index: "graph DB"
    metadata_index: "relational DB or key-value store"
  retrieval:
    semantic_similarity: "cosine distance"
    temporal_filtering: "recency weighting"
    graph_traversal: "relationship following"
    preference_alignment: "user-specific ranking"
    hybrid_fusion: "learned combination of signals"
```

---

## 7. Metadata Enrichment Strategies

### Critical Metadata Dimensions

Based on 2024 research, the following metadata dimensions are essential for memory retrieval:

#### 1. Provenance Metadata
```yaml
provenance:
  created_by: "agent-id or user-id"
  created_at: "timestamp"
  derived_from: ["source event IDs"]
  methodology: "how this memory was created (observed|inferred|taught|learned)"
  confidence: 0.0-1.0
  evidence_count: "number of supporting experiences"
```

#### 2. Usage Metadata
```yaml
usage:
  access_count: "how many times retrieved"
  successful_applications: "times this memory helped solve a problem"
  failed_applications: "times this memory was not helpful"
  success_rate: "successful / total applications"
  last_accessed: "timestamp"
  access_frequency: "exponential moving average"
  contexts_used_in: ["project-A", "feature-B", "debugging"]
```

#### 3. Temporal Metadata
```yaml
temporal:
  timestamp: "ISO-8601 datetime"
  validity_period: "when this knowledge is applicable"
  superseded_by: "newer memory ID if outdated"
  version: "version number if versioned"
  half_life: "expected decay rate"
  recency_score: "computed score based on age and updates"
```

#### 4. Relationship Metadata
```yaml
relationships:
  causal:
    caused_by: ["event IDs"]
    caused: ["event IDs"]
    enables: ["capability IDs"]
    prevents: ["issue IDs"]

  semantic:
    similar_to: ["memory IDs with similarity scores"]
    contradicts: ["conflicting memory IDs"]
    refines: ["memory ID that this improves upon"]
    generalizes: ["memory IDs that this abstracts"]

  temporal:
    before: ["event IDs"]
    after: ["event IDs"]
    concurrent_with: ["event IDs"]

  hierarchical:
    parent: "higher abstraction level memory ID"
    children: ["lower abstraction level memory IDs"]
    level: "abstraction level (0-3)"
```

#### 5. Context Metadata
```yaml
context:
  project: "project identifier"
  domain: "technical domain (frontend|backend|database|devops)"
  task_type: "task category (feature|bugfix|refactor|test)"
  environment: "dev|staging|production"
  tech_stack: ["languages", "frameworks", "tools"]
  complexity: "simple|moderate|complex"
  team_size: "solo|pair|team"
```

#### 6. Quality Metadata
```yaml
quality:
  confidence: 0.0-1.0
  completeness: 0.0-1.0
  consistency: "consistent with other memories"
  verified: true/false
  verification_method: "tested|reviewed|validated"
  reliability: "historical success rate"
  exceptions: ["known cases where this doesn't apply"]
```

#### 7. Preference Metadata
```yaml
preference:
  user_rating: 1-5
  user_feedback: "textual feedback"
  preference_alignment: 0.0-1.0
  personalization_level: "generic|personalized|highly-personalized"
  user_specific: true/false
```

### Metadata Enrichment Techniques

#### 1. LLM-Generated Metadata
```python
# Pseudocode for LLM metadata enrichment
def enrich_memory_metadata(memory_content, context):
    prompt = f"""
    Analyze this memory and extract metadata:

    Memory: {memory_content}
    Context: {context}

    Extract:
    1. Key concepts (tags)
    2. Technical domain
    3. Complexity level
    4. Applicable scenarios
    5. Prerequisites
    6. Potential outcomes
    7. Related concepts
    8. Confidence level
    """

    metadata = llm.generate(prompt)
    return parse_metadata(metadata)
```

#### 2. Composite Embeddings
```python
# Create weighted sum of embeddings from multiple fields
# (2024 best practice from RAG research)

def create_composite_embedding(memory):
    # Embed different fields
    content_emb = embed(memory.content)
    context_emb = embed(memory.context)
    outcome_emb = embed(memory.outcome)

    # Weight by importance
    composite = (
        0.5 * content_emb +
        0.3 * context_emb +
        0.2 * outcome_emb
    )

    # Normalize
    return composite / np.linalg.norm(composite)
```

#### 3. Relationship Extraction
```python
# Extract explicit relationships between memories

def extract_relationships(new_memory, existing_memories):
    relationships = {
        'similar_to': [],
        'caused_by': [],
        'conflicts_with': [],
        'generalizes': []
    }

    for existing in existing_memories:
        # Semantic similarity
        similarity = cosine_similarity(
            new_memory.embedding,
            existing.embedding
        )
        if similarity > 0.8:
            relationships['similar_to'].append(
                (existing.id, similarity)
            )

        # Temporal causality
        if new_memory.timestamp > existing.timestamp:
            if check_causal_link(existing, new_memory):
                relationships['caused_by'].append(existing.id)

        # Contradiction detection
        if detect_contradiction(existing, new_memory):
            relationships['conflicts_with'].append(existing.id)

    return relationships
```

#### 4. Usage Tracking
```python
# Track usage patterns to enrich metadata

class MemoryUsageTracker:
    def on_retrieval(self, memory_id, query, success):
        memory = self.get_memory(memory_id)

        # Update counts
        memory.metadata.access_count += 1
        if success:
            memory.metadata.successful_applications += 1
        else:
            memory.metadata.failed_applications += 1

        # Update success rate
        total = memory.metadata.successful_applications + \
                memory.metadata.failed_applications
        memory.metadata.success_rate = \
            memory.metadata.successful_applications / total

        # Update last accessed
        memory.metadata.last_accessed = datetime.now()

        # Update access frequency (EMA)
        time_delta = datetime.now() - memory.metadata.last_accessed
        memory.metadata.access_frequency = \
            0.9 * memory.metadata.access_frequency + \
            0.1 * (1.0 / time_delta.total_seconds())

        self.save_memory(memory)
```

---

## 8. Query Strategies for Memory Retrieval

### Hybrid Search Architecture

Based on 2024-2025 research, effective memory retrieval requires combining multiple search strategies:

```yaml
hybrid_search_components:
  1_semantic_search:
    method: "dense vector similarity"
    models: "text-embedding-3-large, BGE-M3, or custom"
    use_for: "finding conceptually related memories"
    weight: 0.4

  2_keyword_search:
    method: "BM25 or sparse embeddings"
    use_for: "exact term matching, technical keywords"
    weight: 0.2

  3_temporal_search:
    method: "time-range filtering + recency weighting"
    use_for: "finding recent or time-specific memories"
    weight: 0.15

  4_graph_search:
    method: "relationship traversal"
    use_for: "finding causally or semantically linked memories"
    weight: 0.15

  5_preference_search:
    method: "user preference alignment"
    use_for: "personalizing results"
    weight: 0.1

  fusion_strategy: "learned combination (RankFusion or CombSUM)"
```

### Query Decomposition

For complex queries, decompose into sub-queries and route appropriately (LevelRAG 2024 approach):

```python
def decompose_and_route(complex_query):
    # Example: "How did we solve authentication issues last month,
    #           and what did we learn for next time?"

    sub_queries = llm.decompose(complex_query)
    # Result:
    # 1. "authentication issues last month" â†’ temporal + semantic
    # 2. "what did we learn" â†’ semantic (level_2 patterns)
    # 3. "for next time" â†’ preference-weighted recommendations

    results = []
    for sub_q in sub_queries:
        search_strategy = route_query(sub_q)
        results.append(execute_search(sub_q, search_strategy))

    return combine_results(results)

def route_query(query):
    query_type = classify_query(query)

    if query_type == "temporal":
        return {
            'primary': 'temporal_search',
            'secondary': 'semantic_search',
            'filters': extract_time_range(query)
        }
    elif query_type == "conceptual":
        return {
            'primary': 'semantic_search',
            'secondary': 'graph_search',
            'hierarchy_level': determine_abstraction_level(query)
        }
    elif query_type == "procedural":
        return {
            'primary': 'graph_search',
            'secondary': 'semantic_search',
            'traversal': 'sequence-following'
        }
    # ... other query types
```

### Contextual Experience Replay (CER) Integration

Implement experience replay mechanism for learning from past interactions:

```python
class ExperienceReplayBuffer:
    """
    Based on 2024 research on CER for LLM agents
    """

    def __init__(self, max_size=10000):
        self.buffer = []
        self.max_size = max_size
        self.embeddings = []

    def add_experience(self, task, actions, outcome, learned_pattern):
        experience = {
            'task': task,
            'actions': actions,
            'outcome': outcome,
            'learned_pattern': learned_pattern,
            'timestamp': datetime.now(),
            'embedding': embed(f"{task} {actions} {outcome}")
        }

        self.buffer.append(experience)
        self.embeddings.append(experience['embedding'])

        if len(self.buffer) > self.max_size:
            self.buffer.pop(0)
            self.embeddings.pop(0)

    def retrieve_relevant_experiences(self, current_task, k=5):
        """
        Retrieve experiences similar to current task
        """
        query_embedding = embed(current_task)

        # Compute similarities
        similarities = cosine_similarity(
            query_embedding,
            self.embeddings
        )

        # Get top-k with recency weighting
        weighted_scores = []
        for i, sim in enumerate(similarities):
            recency_weight = compute_recency_weight(
                self.buffer[i]['timestamp']
            )
            weighted_scores.append(sim * recency_weight)

        top_k_indices = np.argsort(weighted_scores)[-k:]

        return [self.buffer[i] for i in top_k_indices]

    def synthesize_pattern(self, experiences):
        """
        Generalize from multiple experiences to extract pattern
        """
        pattern_prompt = f"""
        Given these similar past experiences:
        {format_experiences(experiences)}

        Extract the common pattern and general approach that worked.
        """

        return llm.generate(pattern_prompt)
```

### Adaptive Retrieval

Adjust retrieval strategy based on query characteristics and past success:

```python
class AdaptiveRetriever:
    def __init__(self):
        self.strategy_performance = defaultdict(lambda: {'success': 0, 'total': 0})

    def retrieve(self, query, context):
        # Select strategy based on query type and past performance
        query_type = classify_query_type(query)
        best_strategy = self.select_best_strategy(query_type)

        results = self.execute_strategy(query, best_strategy)

        return results

    def select_best_strategy(self, query_type):
        # Get strategies that have worked well for this query type
        strategies = STRATEGIES_FOR_QUERY_TYPE[query_type]

        # Rank by past performance
        ranked = sorted(
            strategies,
            key=lambda s: self.strategy_performance[s]['success'] /
                         max(self.strategy_performance[s]['total'], 1),
            reverse=True
        )

        # Epsilon-greedy: explore 10% of the time
        if random.random() < 0.1:
            return random.choice(strategies)
        else:
            return ranked[0]

    def report_success(self, strategy, success):
        self.strategy_performance[strategy]['total'] += 1
        if success:
            self.strategy_performance[strategy]['success'] += 1
```

---

## 9. Implementation Recommendations for Learning Loop System

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Learning Loop System                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Memory Ingestion Layer                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Experience Capture (task execution, user feedback)         â”‚
â”‚  â€¢ Metadata Extraction (LLM-powered)                          â”‚
â”‚  â€¢ Relationship Detection (causal, temporal, semantic)        â”‚
â”‚  â€¢ Abstraction Level Assignment                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Embedding Generation Layer                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Semantic Embeddings (content)                              â”‚
â”‚  â€¢ Temporal Embeddings (time-aware)                           â”‚
â”‚  â€¢ Preference Embeddings (user-specific)                      â”‚
â”‚  â€¢ Structural Embeddings (graph position)                     â”‚
â”‚  â€¢ Composite Fusion (learned combination)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Hierarchical Memory Store                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Level 0: Atomic Actions (Vector DB + Time-series DB)         â”‚
â”‚  Level 1: Episodic Tasks (Vector DB + Graph DB)               â”‚
â”‚  Level 2: Semantic Patterns (Graph DB + KG)                   â”‚
â”‚  Level 3: Strategic Principles (KG + Document Store)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Memory Consolidation Service                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Pattern Detection (identify recurring sequences)           â”‚
â”‚  â€¢ Knowledge Distillation (generalize from experiences)       â”‚
â”‚  â€¢ Hierarchy Building (create abstractions)                   â”‚
â”‚  â€¢ Memory Pruning (archive low-value memories)                â”‚
â”‚  â€¢ Relationship Refinement (strengthen/weaken links)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Hybrid Retrieval Engine                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Query Decomposition (break complex queries)                â”‚
â”‚  â€¢ Multi-Strategy Search (semantic + keyword + temporal +     â”‚
â”‚    graph + preference)                                        â”‚
â”‚  â€¢ Adaptive Routing (query-type â†’ best strategy)              â”‚
â”‚  â€¢ Result Fusion (rank aggregation)                           â”‚
â”‚  â€¢ Contextual Experience Replay (relevant past experiences)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Preference Learning Module                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Feedback Collection (explicit + implicit)                  â”‚
â”‚  â€¢ Preference Embedding Learning (VPL approach)               â”‚
â”‚  â€¢ Preference Dimensionality Reduction (PCA/DRM)              â”‚
â”‚  â€¢ Context-Dependent Preferences                              â”‚
â”‚  â€¢ Preference Drift Detection                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Continuous Learning Loop                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Execute Task                                              â”‚
â”‚  2. Capture Experience â†’ Level 0 Memory                       â”‚
â”‚  3. Collect User Feedback â†’ Preference Update                 â”‚
â”‚  4. Retrieve Relevant Past Experiences (CER)                  â”‚
â”‚  5. Apply Learned Patterns                                    â”‚
â”‚  6. Record Outcome                                            â”‚
â”‚  7. Periodic Consolidation â†’ Level 1, 2, 3 Memories           â”‚
â”‚  8. Adapt Retrieval Strategies based on success               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack Recommendations

```yaml
storage_layer:
  vector_database:
    primary_choice: "Weaviate or Qdrant"
    reasoning: "Support for hybrid search, filtering, and multi-vector"
    alternative: "Pinecone or Milvus"

  graph_database:
    primary_choice: "Neo4j"
    reasoning: "Mature, excellent for relationship queries"
    alternative: "Memgraph or JanusGraph"
    use_for: "Episodic relationships, knowledge graphs"

  time_series_database:
    primary_choice: "TimescaleDB (PostgreSQL extension)"
    reasoning: "SQL-compatible, good for temporal queries"
    alternative: "InfluxDB"
    use_for: "Temporal patterns, recency calculations"

  key_value_store:
    primary_choice: "Redis"
    reasoning: "Fast metadata lookups, caching"
    use_for: "Frequently accessed metadata, session state"

embedding_models:
  semantic:
    model: "text-embedding-3-large or BGE-M3"
    dimension: 1024-3072
    use_for: "Content semantic similarity"

  temporal:
    approach: "Custom T-Rep style time-embeddings"
    dimension: 128
    use_for: "Time-aware retrieval"

  preference:
    approach: "Learned user embedding (VPL style)"
    dimension: 64
    use_for: "Personalization"

  structural:
    approach: "Node2Vec or Graph Attention Networks"
    dimension: 64
    use_for: "Graph position encoding"

llm_integration:
  metadata_extraction: "Claude Sonnet or GPT-4"
  pattern_synthesis: "Claude Sonnet or GPT-4"
  query_decomposition: "Claude Haiku (fast) or Sonnet (complex)"
  contextual_summarization: "Claude Sonnet"

frameworks:
  orchestration: "LangChain or LlamaIndex for RAG pipeline"
  memory_management: "Custom with MemGPT or Memary inspiration"
  graph_operations: "PyG (PyTorch Geometric) for GNNs"
```

### Implementation Phases

```yaml
phase_1_foundation:
  duration: "2-3 weeks"
  components:
    - basic_vector_storage: "Weaviate with single embedding type"
    - simple_metadata: "timestamp, source, type"
    - semantic_retrieval: "cosine similarity"
    - level_0_memory: "atomic actions only"
  deliverable: "Can store and retrieve task execution memories"

phase_2_temporal:
  duration: "1-2 weeks"
  components:
    - temporal_embeddings: "add time-aware representations"
    - recency_weighting: "decay functions"
    - time_range_filtering: "enable temporal queries"
  deliverable: "Can retrieve recent and time-specific memories"

phase_3_hierarchy:
  duration: "2-3 weeks"
  components:
    - multi_level_storage: "levels 0-3"
    - consolidation_service: "periodic pattern extraction"
    - hierarchical_retrieval: "adaptive depth search"
  deliverable: "System learns patterns and builds knowledge"

phase_4_graph:
  duration: "2-3 weeks"
  components:
    - graph_database: "Neo4j integration"
    - relationship_extraction: "causal, semantic, temporal links"
    - graph_search: "relationship traversal"
  deliverable: "Can answer relational queries (why, how, what led to)"

phase_5_preferences:
  duration: "1-2 weeks"
  components:
    - feedback_collection: "explicit and implicit"
    - preference_embeddings: "user-specific vectors"
    - personalized_retrieval: "preference-weighted ranking"
  deliverable: "System adapts to user preferences"

phase_6_hybrid:
  duration: "2-3 weeks"
  components:
    - hybrid_search: "semantic + keyword + temporal + graph + preference"
    - query_routing: "adaptive strategy selection"
    - result_fusion: "rank aggregation"
  deliverable: "Production-ready retrieval system"

phase_7_continuous_learning:
  duration: "2-3 weeks"
  components:
    - experience_replay: "CER implementation"
    - adaptive_retrieval: "strategy performance tracking"
    - automatic_consolidation: "background pattern extraction"
  deliverable: "Fully autonomous learning loop"
```

### Code Example: Complete Memory Storage

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict, Optional
import numpy as np

@dataclass
class MemoryMetadata:
    """Rich metadata for memory"""
    # Provenance
    created_by: str
    created_at: datetime
    derived_from: List[str]
    confidence: float

    # Usage
    access_count: int = 0
    successful_applications: int = 0
    failed_applications: int = 0
    success_rate: float = 0.0
    last_accessed: Optional[datetime] = None

    # Temporal
    validity_period: Optional[tuple[datetime, datetime]] = None
    superseded_by: Optional[str] = None
    version: int = 1

    # Relationships
    relationships: Dict[str, List[str]] = None

    # Context
    project: str = ""
    domain: str = ""
    task_type: str = ""
    tech_stack: List[str] = None

    # Hierarchy
    level: int = 0  # 0=atomic, 1=episodic, 2=semantic, 3=strategic
    parent: Optional[str] = None
    children: List[str] = None

@dataclass
class Memory:
    """Complete memory representation"""
    id: str
    content: str
    embedding: np.ndarray
    temporal_embedding: np.ndarray
    preference_embedding: np.ndarray
    structural_embedding: np.ndarray
    composite_embedding: np.ndarray
    metadata: MemoryMetadata

class MemorySystem:
    """Complete memory system implementation"""

    def __init__(
        self,
        vector_db,
        graph_db,
        timeseries_db,
        embedding_model,
        temporal_encoder,
        preference_learner
    ):
        self.vector_db = vector_db
        self.graph_db = graph_db
        self.timeseries_db = timeseries_db
        self.embedding_model = embedding_model
        self.temporal_encoder = temporal_encoder
        self.preference_learner = preference_learner

    def store_experience(
        self,
        content: str,
        context: Dict,
        user_id: str
    ) -> Memory:
        """Store a new experience as Level 0 memory"""

        # Generate ID
        memory_id = generate_uuid()

        # Extract metadata using LLM
        metadata = self._extract_metadata(content, context)
        metadata.created_by = user_id
        metadata.created_at = datetime.now()
        metadata.level = 0

        # Generate embeddings
        semantic_emb = self.embedding_model.encode(content)
        temporal_emb = self.temporal_encoder.encode(
            datetime.now(),
            context.get('sequence_position')
        )
        preference_emb = self.preference_learner.get_user_embedding(user_id)

        # Composite embedding (learned fusion)
        composite_emb = self._fuse_embeddings(
            semantic_emb,
            temporal_emb,
            preference_emb,
            np.zeros(64)  # structural embedding comes later
        )

        # Create memory
        memory = Memory(
            id=memory_id,
            content=content,
            embedding=semantic_emb,
            temporal_embedding=temporal_emb,
            preference_embedding=preference_emb,
            structural_embedding=np.zeros(64),
            composite_embedding=composite_emb,
            metadata=metadata
        )

        # Store in vector DB
        self.vector_db.insert(
            id=memory_id,
            vector=composite_emb,
            payload={
                'content': content,
                'metadata': metadata.__dict__,
                'level': 0
            }
        )

        # Store in graph DB
        self.graph_db.create_node(
            id=memory_id,
            label='Experience',
            properties={'content': content, **metadata.__dict__}
        )

        # Extract and create relationships
        relationships = self._extract_relationships(memory, context)
        for rel_type, targets in relationships.items():
            for target_id in targets:
                self.graph_db.create_edge(
                    source=memory_id,
                    target=target_id,
                    relationship_type=rel_type
                )

        # Store temporal data
        self.timeseries_db.insert(
            timestamp=metadata.created_at,
            memory_id=memory_id,
            data={'content': content, 'context': context}
        )

        return memory

    def retrieve_memories(
        self,
        query: str,
        user_id: str,
        strategy: str = 'hybrid',
        k: int = 5
    ) -> List[Memory]:
        """Retrieve relevant memories using hybrid search"""

        if strategy == 'hybrid':
            return self._hybrid_search(query, user_id, k)
        elif strategy == 'semantic':
            return self._semantic_search(query, k)
        elif strategy == 'temporal':
            return self._temporal_search(query, k)
        elif strategy == 'graph':
            return self._graph_search(query, k)
        else:
            raise ValueError(f"Unknown strategy: {strategy}")

    def _hybrid_search(
        self,
        query: str,
        user_id: str,
        k: int
    ) -> List[Memory]:
        """Hybrid search combining multiple strategies"""

        # 1. Semantic search (weight: 0.4)
        query_emb = self.embedding_model.encode(query)
        semantic_results = self.vector_db.search(
            vector=query_emb,
            limit=k * 3
        )
        semantic_scores = {r.id: 0.4 * r.score for r in semantic_results}

        # 2. Keyword search (weight: 0.2)
        keyword_results = self.vector_db.keyword_search(
            query=query,
            limit=k * 3
        )
        keyword_scores = {r.id: 0.2 * r.score for r in keyword_results}

        # 3. Temporal search (weight: 0.15)
        recency_scores = {}
        for result in semantic_results:
            age_days = (datetime.now() - result.metadata.created_at).days
            recency_score = np.exp(-0.1 * age_days)  # exponential decay
            recency_scores[result.id] = 0.15 * recency_score

        # 4. Graph search (weight: 0.15)
        # Find memories connected to recently accessed memories
        recent_memories = self._get_recent_user_memories(user_id, limit=5)
        graph_scores = {}
        for mem_id in recent_memories:
            connected = self.graph_db.get_neighbors(
                node_id=mem_id,
                relationship_types=['similar_to', 'caused', 'enables']
            )
            for neighbor_id in connected:
                graph_scores[neighbor_id] = graph_scores.get(neighbor_id, 0) + 0.15

        # 5. Preference alignment (weight: 0.1)
        user_pref_emb = self.preference_learner.get_user_embedding(user_id)
        preference_scores = {}
        for result in semantic_results:
            alignment = np.dot(user_pref_emb, result.preference_embedding)
            preference_scores[result.id] = 0.1 * alignment

        # Combine scores
        all_memory_ids = set(
            list(semantic_scores.keys()) +
            list(keyword_scores.keys()) +
            list(recency_scores.keys()) +
            list(graph_scores.keys()) +
            list(preference_scores.keys())
        )

        combined_scores = {}
        for mem_id in all_memory_ids:
            combined_scores[mem_id] = (
                semantic_scores.get(mem_id, 0) +
                keyword_scores.get(mem_id, 0) +
                recency_scores.get(mem_id, 0) +
                graph_scores.get(mem_id, 0) +
                preference_scores.get(mem_id, 0)
            )

        # Sort and return top-k
        top_k_ids = sorted(
            combined_scores.keys(),
            key=lambda x: combined_scores[x],
            reverse=True
        )[:k]

        return [self._load_memory(mem_id) for mem_id in top_k_ids]

    def consolidate_memories(self):
        """Periodic consolidation to build hierarchy"""

        # Get recent Level 0 memories
        recent_experiences = self.vector_db.search(
            filter={'level': 0, 'consolidated': False},
            limit=1000
        )

        # Cluster related experiences
        clusters = self._cluster_experiences(recent_experiences)

        # For each cluster, create Level 1 episodic memory
        for cluster in clusters:
            if len(cluster) >= 3:  # Minimum cluster size
                episodic_memory = self._create_episodic_memory(cluster)

                # Check if this reveals a pattern (Level 2)
                pattern = self._detect_pattern(episodic_memory)
                if pattern:
                    semantic_memory = self._create_semantic_memory(pattern)

                    # Check if this refines a principle (Level 3)
                    principle = self._refine_principle(semantic_memory)
                    if principle:
                        self._update_strategic_memory(principle)

        # Mark as consolidated
        for exp in recent_experiences:
            exp.metadata.consolidated = True
            self._update_memory(exp)

    def learn_from_feedback(
        self,
        memory_id: str,
        feedback: str,
        feedback_type: str,  # 'positive', 'negative', 'correction'
        user_id: str
    ):
        """Update preference embeddings based on feedback"""

        # Store feedback event
        feedback_memory = self.store_experience(
            content=f"Feedback on {memory_id}: {feedback}",
            context={
                'type': 'feedback',
                'target_memory': memory_id,
                'feedback_type': feedback_type
            },
            user_id=user_id
        )

        # Update preference embeddings
        self.preference_learner.update_preferences(
            user_id=user_id,
            feedback_type=feedback_type,
            target_memory_id=memory_id,
            feedback_content=feedback
        )

        # Update target memory's metadata
        target_memory = self._load_memory(memory_id)
        if feedback_type == 'positive':
            target_memory.metadata.successful_applications += 1
        elif feedback_type == 'negative':
            target_memory.metadata.failed_applications += 1

        target_memory.metadata.success_rate = (
            target_memory.metadata.successful_applications /
            (target_memory.metadata.successful_applications +
             target_memory.metadata.failed_applications)
        )

        self._update_memory(target_memory)

    def _extract_metadata(self, content: str, context: Dict) -> MemoryMetadata:
        """Use LLM to extract rich metadata"""

        prompt = f"""
        Analyze this memory and extract metadata:

        Content: {content}
        Context: {context}

        Extract and return as JSON:
        - domain: technical domain (frontend/backend/database/devops/etc)
        - task_type: task category (feature/bugfix/refactor/test/etc)
        - tech_stack: list of technologies/frameworks/tools mentioned
        - complexity: simple/moderate/complex
        - key_concepts: list of important concepts
        - confidence: 0.0-1.0 how certain is this information
        """

        llm_response = self.llm.generate(prompt)
        extracted = parse_json(llm_response)

        return MemoryMetadata(
            created_by="",  # Set by caller
            created_at=datetime.now(),
            derived_from=context.get('source_events', []),
            confidence=extracted.get('confidence', 0.8),
            domain=extracted.get('domain', ''),
            task_type=extracted.get('task_type', ''),
            tech_stack=extracted.get('tech_stack', []),
            relationships={}
        )

    def _fuse_embeddings(
        self,
        semantic: np.ndarray,
        temporal: np.ndarray,
        preference: np.ndarray,
        structural: np.ndarray
    ) -> np.ndarray:
        """Learned fusion of multiple embedding types"""

        # In production, this would be a learned neural network
        # For now, weighted concatenation
        concatenated = np.concatenate([
            semantic * 0.5,
            temporal * 0.2,
            preference * 0.15,
            structural * 0.15
        ])

        # Normalize
        return concatenated / np.linalg.norm(concatenated)
```

---

## 10. Concrete Recommendations Summary

### For Task Execution Experiences (Episodic Memory)

1. **Use graph-based storage** (Neo4j or similar) for explicit event relationships
2. **Implement event boundary detection** using LLM-based meta-chunking
3. **Store causal relationships** (what caused what, what enabled what)
4. **Add contextual summaries** using Claude's Contextual Retrieval approach
5. **Enable temporal queries** with time-series database integration
6. **Granularity:** Per-action for details, per-task for retrieval

### For Learned Knowledge (Semantic Memory)

1. **Build hierarchical knowledge graphs** with abstraction levels
2. **Use ReaLM-style quantization** for efficient KG embedding integration
3. **Implement Graphiti-style temporal KGs** for evolving knowledge
4. **Extract patterns from episodic clusters** using periodic consolidation
5. **Store provenance links** from patterns back to source experiences
6. **Granularity:** Per-concept, with cross-references between related concepts

### For User Preferences

1. **Implement VPL-style probabilistic embeddings** for user preferences
2. **Use PCA/DRM** to decompose preferences into interpretable dimensions
3. **Track both explicit and implicit feedback**
4. **Learn context-dependent preferences** (different preferences in different situations)
5. **Enable preference drift detection** and adaptation
6. **Granularity:** Per-user with dimension-wise tracking

### For SOPs and Workflows

1. **Store as Level 2 semantic memories** with structured graph representation
2. **Use DAG representation** for task dependencies and temporal ordering
3. **Link to successful execution examples** (Level 1 episodic memories)
4. **Enable workflow comparison** through structural similarity
5. **Support versioning** as workflows evolve
6. **Granularity:** Per-step with explicit sequencing

### For Decision Trails

1. **Use graph structure** to represent decision trees and paths taken
2. **Store decision rationale** as contextual metadata
3. **Link decisions to outcomes** for learning
4. **Enable "why did we decide X?" queries** through graph traversal
5. **Track decision confidence** and update based on outcomes
6. **Granularity:** Per-decision with full context

### General Best Practices

1. **Start simple** (semantic + temporal) then add graph and preferences
2. **Use hybrid search** from day one (semantic + keyword minimum)
3. **Implement consolidation early** to prevent memory bloat
4. **Track usage metadata** to identify valuable vs. low-value memories
5. **Enable explainability** through provenance tracking
6. **Design for evolution** - memories should update as you learn more
7. **Use LLMs for metadata extraction** - much better than rule-based
8. **Implement experience replay** (CER) for continuous learning
9. **Monitor and adapt retrieval strategies** based on success rates
10. **Build hierarchically** - atomic â†’ episodic â†’ semantic â†’ strategic

---

## 11. Research Papers and Resources

### 2024-2025 Key Papers

1. **MemTree** - "From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs" (2024)
2. **H-MEM** - "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents" (2024)
3. **Contextual Experience Replay** - "Contextual Experience Replay for Self-Improvement of Language Agents" (2024)
4. **T-Rep** - "T-Rep: Representation Learning for Time Series using Time-Embeddings" (2024)
5. **TEMPO** - ICLR 2024
6. **ReaLM** - "Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models" (2024)
7. **Graphiti** - Temporal Knowledge Graph system (2024)
8. **AgentRR** - "Get Experience from Practice: LLM Agents with Record & Replay" (2025)
9. **Hierarchical Embedding Augmentation** - "Autonomous Structural Memory Manipulation for Large Language Models" (2025)
10. **VPL** - "Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning" (2024)

### Tools and Frameworks

- **Weaviate** - Vector database with hybrid search
- **Neo4j** - Graph database for relationships
- **Graphiti** - Temporal knowledge graph library
- **LangChain** / **LlamaIndex** - RAG orchestration
- **MemGPT** - Memory management for LLM agents
- **BGE-M3** / **text-embedding-3** - Embedding models

---

## Conclusion

Memory embeddings for learning loop systems require fundamentally different approaches than standard document embeddings. The key differences are:

1. **Multi-dimensional representation** (semantic + temporal + preference + structural)
2. **Hierarchical organization** (atomic â†’ episodic â†’ semantic â†’ strategic)
3. **Dynamic relationships** (explicit graph structure)
4. **Continuous learning** (experience replay, consolidation)
5. **Personalization** (user-specific preference embeddings)

The 2024-2025 research shows clear convergence toward:
- Hierarchical memory architectures (MemTree, H-MEM)
- Temporal knowledge graphs (Graphiti)
- Experience replay for agents (CER, AgentRR)
- Hybrid retrieval strategies
- LLM-powered metadata enrichment

**Recommendation:** Start with a hybrid semantic + temporal system, add hierarchical organization, then graph relationships, and finally preference learning. This phased approach balances complexity with capability while building toward a fully autonomous learning loop system.

All file paths referenced:
- /home/aepod/dev/weave-nn/weave-nn/docs/research/memorographic-embeddings-research.md
